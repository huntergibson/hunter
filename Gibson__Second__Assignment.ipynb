{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/huntergibson/hunter/blob/main/Gibson__Second__Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CS3033/CS6405 - Data Mining - Second Assignment\n",
        "\n",
        "### Submission\n",
        "\n",
        "This assignment is **due on 07/04/22 at 23:59**. You should submit a single .ipnyb file with your python code and analysis electronically via Canvas.\n",
        "Please note that this assignment will account for 25 Marks of your module grade.\n",
        "\n",
        "### Declaration\n",
        "\n",
        "By submitting this assignment. I agree to the following:\n",
        "\n",
        "<font color=\"red\">“I have read and understand the UCC academic policy on plagiarism, and agree to the requirements set out thereby in relation to plagiarism and referencing. I confirm that I have referenced and acknowledged properly all sources used in the preparation of this assignment.\n",
        "I declare that this assignment is entirely my own work based on my personal study. I further declare that I have not engaged the services of another to either assist me in, or complete this assignment”</font>\n",
        "\n",
        "### Objective\n",
        "\n",
        "The Boolean satisfiability (SAT) problem consists in determining whether a Boolean formula F is satisfiable or not. F is represented by a pair (X, C), where X is a set of Boolean variables and C is a set of clauses in Conjunctive Normal Form (CNF). Each clause is a disjunction of literals (a variable or its negation). This problem is one of the most widely studied combinatorial problems in computer science. It is the classic NP-complete problem. Over the past number of decades, a significant amount of research work has focused on solving SAT problems with both complete and incomplete solvers.\n",
        "\n",
        "One of the most successful approaches is an algorithm portfolio, where a solver is selected among a set of candidates depending on the problem type. Your task is to create a classifier that takes as input the SAT instance's features and identifies the class.\n",
        "\n",
        "In this project, we represent SAT problems with a vector of 327 features with general information about the problem, e.g., number of variables, number of clauses, the fraction of horn clauses in the problem, etc. There is no need to understand the features to be able to complete the assignment.\n",
        "\n",
        "\n",
        "The original dataset is available at:\n",
        "https://github.com/bprovanbessell/SATfeatPy/blob/main/features_csv/all_features.csv\n",
        "\n"
      ],
      "metadata": {
        "id": "8WfrCFmLHxYu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "Oav9G1WSJ1nH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/andvise/DataAnalyticsDatasets/main/train_dataset.csv\", index_col=0)\n",
        "df.head()\n",
        "df = df.fillna(0)"
      ],
      "metadata": {
        "id": "DE0kM0QsJ1En"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Label or target variable\n",
        "df['target'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8MCvTYTKw4Q",
        "outputId": "b729937a-c9ae-4bb1-80f3-43bf2165e0d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tseitin           298\n",
              "dominating        294\n",
              "cliquecoloring    268\n",
              "php               266\n",
              "subsetcard        263\n",
              "op                201\n",
              "tiling            120\n",
              "5clique           108\n",
              "3color            104\n",
              "matching          102\n",
              "5color             98\n",
              "4color             98\n",
              "3clique            98\n",
              "4clique            94\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tasks\n",
        "\n",
        "## Basic models and evaluation (5 Marks)\n",
        "\n",
        "Using Scikit-learn, train and evaluate a decision tree classifier using 70% of the dataset from training and 30% for testing. For this part of the project, we are not interested in optimising the parameters; we just want to get an idea of the dataset."
      ],
      "metadata": {
        "id": "MTvkBPQvITf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "X = df.iloc[:,:-1]\n",
        "y = df.iloc[:,-1]"
      ],
      "metadata": {
        "id": "Zl0VXO0YH1nG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates encoder for target\n",
        "from sklearn import preprocessing\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "y = encoder.fit_transform(y)"
      ],
      "metadata": {
        "id": "Jz-1U9t3ZkBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train - Test Split\n",
        "from sklearn import model_selection\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(X,y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "NWD6JQe8Z1V-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prints shape of datasets\n",
        "print(\"Train shape:\", X_train.shape, y_train.shape)\n",
        "print(\"Test shape:\", X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ewoj7Ui_aNBf",
        "outputId": "46897886-57dd-4acd-9bea-638a3ea8124c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (1688, 327) (1688,)\n",
            "Test shape: (724, 327) (724,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace with zero\n",
        "import numpy as np\n",
        "X_train.replace([np.inf, -np.inf], 0, inplace=True)\n",
        "X_test.replace([np.inf, -np.inf], 0, inplace=True)"
      ],
      "metadata": {
        "id": "2fvKx4h0acse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check null cells\n",
        "df.isnull().sum().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8YAqPsFay1g",
        "outputId": "7c932dfc-693e-4a39-9001-f1e766676483"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import tree\n",
        "\n",
        "# Create a decision tree classifier\n",
        "cls = tree.DecisionTreeClassifier(random_state = 42)\n",
        "\n",
        "# Train tree on training data\n",
        "cls.fit(X_train, y_train)\n",
        "\n",
        "# Check accuracy on test data\n",
        "cls.score(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJBF4FZOa8Iz",
        "outputId": "813cb7b0-ce58-4f97-f8fa-791a75f5ad3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.988950276243094"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Robust evaluation (10 Marks)\n",
        "\n",
        "In this section, we are interested in more rigorous techniques by implementing more sophisticated methods, for instance:\n",
        "* Hold-out and cross-validation.\n",
        "* Hyper-parameter tuning.\n",
        "* Feature reduction.\n",
        "* Feature selection.\n",
        "* Feature normalisation.\n",
        "\n",
        "Your report should provide concrete information about your reasoning; everything should be well-explained.\n",
        "\n",
        "The key to geting good marks is to show that you evaluated different methods and that you correctly selected the configuration."
      ],
      "metadata": {
        "id": "zADpr0f8IcGL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###I first started by minmax scaling the data to prevent large features from overshadowing the small ones. I then decided to define a set of hyperparameters and use cross-validation in order to improve model performance and reduce overfitting of the data. After printing the score from this result I got an accuracy of 99.5%.I then tried to run a decision tree on the data which did not improve the accuracy as it only got an accuracy of 98.3%. So I decided to just move on with the knn regressor."
      ],
      "metadata": {
        "id": "NOoXlq0YeHwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates minmax scaler\n",
        "scaler = preprocessing.MinMaxScaler()\n",
        "\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "tvBZH6ilInsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates parameters \n",
        "parameters = {'n_neighbors': range(1,328), 'p': [1,2], 'weights': ['uniform', 'distance']}"
      ],
      "metadata": {
        "id": "PnEMpgiBbhpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import neighbors\n",
        "\n",
        "knn = neighbors.KNeighborsRegressor()\n",
        "\n",
        "reg = model_selection.GridSearchCV(knn, parameters)\n",
        "reg.fit(X_train, y_train)\n",
        "print('The best classifier is:', reg.best_estimator_)\n",
        "print('Its accuracy is:', reg.best_score_)\n",
        "print('Its parameters are:', reg.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcFDxFTcb2IX",
        "outputId": "25f246b6-9f7a-4aa9-b078-0dea52f23159"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best classifier is: KNeighborsRegressor(n_neighbors=1, p=1)\n",
            "Its accuracy is: 0.9949851820277482\n",
            "Its parameters are: {'n_neighbors': 1, 'p': 1, 'weights': 'uniform'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import tree\n",
        "\n",
        "# Create a decision tree classifier\n",
        "cls = tree.DecisionTreeClassifier(random_state = 42)\n",
        "\n",
        "# Train tree on training data\n",
        "cls.fit(X_train, y_train)\n",
        "\n",
        "# Check accuracy on test data\n",
        "cls.score(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtFluK47HCzz",
        "outputId": "ba94c208-2307-44bb-d756-5f7df118d5ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9834254143646409"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zfd7mU2DeEHW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## New classifier (10 Marks)\n",
        "\n",
        "Replicate the previous task for a classifier different than K-NN and decision trees. Briefly describe your choice.\n",
        "Try to create the best model for the given dataset.\n",
        "\n",
        "\n",
        "Save your best model into your github. And create a single code cell that loads it and evaluate it on the following test dataset:\n",
        "https://github.com/andvise/DataAnalyticsDatasets/blob/main/test_dataset.csv\n",
        "\n",
        "This link currently contains a sample of the training set. The real test set will be released after the submission. I should be able to run the code cell independently, load all the libraries you need as well."
      ],
      "metadata": {
        "id": "FYoMg0EZIrNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###In this first model the classifier I used was Logistic Regression. Logistic regression is very efficient and quick to classify unknown data points. After fitting the dataset and scoring the model the accuracy was 98.2%. Which was lower that the 99.4% using knn so I decided to try another classifier technique. I chose Random Forest classifier next as this deals with majority voting to come up with a result and can maybe give me a better accuracy. After fitting the classifier on the data it returned perfect accuracy of 100%. After seeing this result I chose the Random Forest classifier model as my best model."
      ],
      "metadata": {
        "id": "GCVO_6oagUZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Create classifier\n",
        "cls = LogisticRegression(random_state = 42)\n",
        "\n",
        "# Train classifier\n",
        "cls.fit(X_train, y_train)\n",
        "\n",
        "# Check accuracy\n",
        "cls.score(X_test, y_test)"
      ],
      "metadata": {
        "id": "QRJXrY2hI32F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c6ec0a2-8221-40f8-b988-1c11e7b2ddc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9820441988950276"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Create classifier\n",
        "cls = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train classifier\n",
        "cls.fit(X_train, y_train)\n",
        "\n",
        "# Check Accuracy\n",
        "cls.score(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-dIBlWfddBv",
        "outputId": "fffacad6-d367-48e8-dd90-d12523938886"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"blue\">FOR GRADING ONLY</font>\n",
        "\n",
        "Save your best model into your github. And create a single code cell that loads it and evaluate it on the following test dataset: \n",
        "https://github.com/andvise/DataAnalyticsDatasets/blob/main/test_dataset.csv\n"
      ],
      "metadata": {
        "id": "Q01BjiiCJTR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import dump, load\n",
        "from io import BytesIO\n",
        "import requests\n",
        "\n",
        "# INSERT YOUR MODEL'S URL\n",
        "mLink = 'URL_OF_YOUR_MODEL_SAVED_IN_YOUR_GITHUB_REPOSITORY?raw=true'\n",
        "mfile = BytesIO(requests.get(mLink).content)\n",
        "model = load(mfile)\n",
        "\n",
        "# Loads dataset\n",
        "df = pd.read_csv(\"https://github.com/andvise/DataAnalyticsDatasets/blob/main/test_dataset.csv?raw=true\", index_col=0)\n",
        "df = df.replace([np.inf, -np.inf], 0)\n",
        "df = df.fillna(0)\n",
        "X = df.iloc[:,:-1]\n",
        "y = df.iloc[:,-1]\n",
        "\n",
        "# Model evaluation\n",
        "# Creates encoder for target\n",
        "from sklearn import preprocessing\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "y = encoder.fit_transform(y)\n",
        "# Train - Test Split\n",
        "from sklearn import model_selection\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(X,y, test_size=0.3, random_state=42)\n",
        "# Fill with zero\n",
        "import numpy as np\n",
        "X_train.replace([np.inf, -np.inf], 0, inplace=True)\n",
        "X_test.replace([np.inf, -np.inf], 0, inplace=True)\n",
        "from sklearn import tree\n",
        "# Create a decision tree classifier\n",
        "cls = tree.DecisionTreeClassifier(random_state = 42)\n",
        "# Train tree on training data\n",
        "cls.fit(X_train, y_train)\n",
        "# Creates minmax scaler\n",
        "scaler = preprocessing.MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# Create classifier\n",
        "cls = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "# Train classifier\n",
        "cls.fit(X_train, y_train)\n",
        "# Check Accuracy\n",
        "cls.score(X_test, y_test)"
      ],
      "metadata": {
        "id": "IWx4lyuQI929"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u1UD4qcndb5x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}